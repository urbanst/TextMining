{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time \n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#opening the files\n",
    "import pandas as pd\n",
    "import slate3k as slate\n",
    "import os\n",
    "\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re  \n",
    "\n",
    "# saving list of documents as file\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize some global variables\n",
    "# initializing bad_chars_list --> another option to remove bad chars x = s.replace(\"'\", \"\") --> leaves empty space where removed\n",
    "bad_chars = ['_','(',')','√','-']\n",
    "\n",
    "# maribor\n",
    "documents_um = {}\n",
    "documents_um_feri = {}\n",
    "documents_um_fzv = {}\n",
    "documents_um_pef = {}\n",
    "\n",
    "# ljubljana\n",
    "documents_ul = {}\n",
    "documents_ul_pef = {}\n",
    "documents_ul_fri = {}\n",
    "documents_ul_fzv = {}\n",
    "\n",
    "# univerza na primorskem\n",
    "documents_up = {}\n",
    "documents_up_pef = {}\n",
    "documents_up_famnit = {}\n",
    "\n",
    "# univerza novo mesto\n",
    "documents_unnm_fpuv = {}\n",
    "documents_unnm_fzv = {}\n",
    "\n",
    "documents_unnm_fei = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_common=timer()\n",
    "#################################### -- UNIVERZA MARIBOR DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUM') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of some bad chars\n",
    "        documents_um[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "um_corpus_all_in_one = ' '.join([doc for doc in documents_um.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning, lets try define regex\n",
    "cleaned_text_um = um_corpus_all_in_one.lower()  \n",
    "#cleaned_text_um = re.sub('[^a-žA-Ž]', ' ', cleaned_text_um )  \n",
    "cleaned_text_um = re.sub(r'\\s+', ' ', cleaned_text_um)\n",
    "end=timer()\n",
    "print(\"time required for reading UM files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "#################################### -- UNIVERZA MARIBOR, FERI DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUM_Feri') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of some bad chars\n",
    "        documents_um_feri[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "um_feri_corpus_all_in_one = ' '.join([doc for doc in documents_um_feri.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning, lets try define regex\n",
    "cleaned_text_um_feri = um_feri_corpus_all_in_one.lower()  \n",
    "#cleaned_text_um_feri = re.sub('[^a-žA-Ž]', ' ', cleaned_text_um_feri )  \n",
    "cleaned_text_um_feri = re.sub(r'\\s+', ' ', cleaned_text_um_feri)\n",
    "end=timer()\n",
    "print(\"time required for reading UM FERI files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "#################################### -- UNIVERZA MARIBOR, FZV DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUM_FZV') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of some bad chars\n",
    "        documents_um_fzv[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "um_fzv_corpus_all_in_one = ' '.join([doc for doc in documents_um_fzv.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning, lets try define regex\n",
    "cleaned_text_um_fzv = um_fzv_corpus_all_in_one.lower()  \n",
    "#cleaned_text_um_fzv = re.sub('[^a-žA-Ž]', ' ', cleaned_text_um_fzv )  \n",
    "cleaned_text_um_fzv = re.sub(r'\\s+', ' ', cleaned_text_um_fzv)\n",
    "end=timer()\n",
    "print(\"time required for reading UM FZV files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "#################################### -- UNIVERZA MARIBOR, PEF DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUM_PEF') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of some bad chars\n",
    "        documents_um_pef[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "um_pef_corpus_all_in_one = ' '.join([doc for doc in documents_um_pef.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning, lets try define regex\n",
    "cleaned_text_um_pef = um_pef_corpus_all_in_one.lower()  \n",
    "#cleaned_text_um_pef = re.sub('[^a-žA-Ž]', ' ', cleaned_text_um_pef )  \n",
    "cleaned_text_um_pef = re.sub(r'\\s+', ' ', cleaned_text_um_pef)\n",
    "end=timer()\n",
    "print(\"time required for reading UM PEF files:\", end-start)\n",
    "#############################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### -- UNIVERZA LJUBLJANA DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUL') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_ul[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "ul_corpus_all_in_one = ' '.join([doc for doc in documents_ul.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_ul = ul_corpus_all_in_one.lower()  \n",
    "#cleaned_text_ul = re.sub('[^a-žA-Ž]', ' ', cleaned_text_ul )  \n",
    "cleaned_text_ul = re.sub(r'\\s+', ' ', cleaned_text_ul)\n",
    "end=timer()\n",
    "print(\"time required for reading UL files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "#################################### -- UNIVERZA LJUBLJANA, PEDAGOŠKA FAKULTETA DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUL_PEF') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_ul_pef[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "ul_pef_corpus_all_in_one = ' '.join([doc for doc in documents_ul_pef.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_ul_pef = ul_pef_corpus_all_in_one.lower()  \n",
    "#cleaned_text_ul_pef = re.sub('[^a-žA-Ž]', ' ', cleaned_text_ul_pef )  \n",
    "cleaned_text_ul_pef = re.sub(r'\\s+', ' ', cleaned_text_ul_pef)\n",
    "end=timer()\n",
    "print(\"time required for reading UL PEF files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "#################################### -- UNIVERZA LJUBLJANA, FRI DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUL_FRI') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_ul_fri[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "ul_fri_corpus_all_in_one = ' '.join([doc for doc in documents_ul_fri.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_ul_fri = ul_fri_corpus_all_in_one.lower()  \n",
    "#cleaned_text_ul_fri = re.sub('[^a-žA-Ž]', ' ', cleaned_text_ul_fri )  \n",
    "cleaned_text_ul_fri = re.sub(r'\\s+', ' ', cleaned_text_ul_fri)\n",
    "end=timer()\n",
    "print(\"time required for reading UL FRI files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "#################################### -- UNIVERZA LJUBLJANA, FAKULTETA ZA ZDRAVSTVENE VEDE DATA FOLDER -- ################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUL_ZF') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_ul_fzv[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "ul_fzv_corpus_all_in_one = ' '.join([doc for doc in documents_ul_fzv.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_ul_fzv = ul_fzv_corpus_all_in_one.lower()  \n",
    "#cleaned_text_ul_fzv = re.sub('[^a-žA-Ž]', ' ', cleaned_text_ul_fzv )  \n",
    "cleaned_text_ul_fzv = re.sub(r'\\s+', ' ', cleaned_text_ul_fzv)\n",
    "end=timer()\n",
    "print(\"time required for reading UL ZF files:\", end-start)\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### -- UNIVERZA NA PRIMORSKEM -- ###########################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUp') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_up[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "up_corpus_all_in_one = ' '.join([doc for doc in documents_up.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_up = up_corpus_all_in_one.lower()  \n",
    "#cleaned_text_up = re.sub('[^a-žA-Ž]', ' ', cleaned_text_up )  \n",
    "cleaned_text_up = re.sub(r'\\s+', ' ', cleaned_text_up)\n",
    "end=timer()\n",
    "print(\"time required for reading UP files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "#################################### -- UNIVERZA NA PRIMORSKEM, PEDAGOŠKA FAKULTETA DATA FOLDER -- ###########################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUP_PEF') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_up_pef[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "up_pef_corpus_all_in_one = ' '.join([doc for doc in documents_up_pef.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_up_pef = up_pef_corpus_all_in_one.lower()  \n",
    "#cleaned_text_up_pef = re.sub('[^a-žA-Ž]', ' ', cleaned_text_up_pef )  \n",
    "cleaned_text_up_pef = re.sub(r'\\s+', ' ', cleaned_text_up_pef)\n",
    "end=timer()\n",
    "print(\"time required for reading UP PEF files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "#################################### -- UNIVERZA NA PRIMORSKEM, FAMNIT DATA FOLDER -- ###########################################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUP_FAMNIT') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_up_famnit[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "up_famnit_corpus_all_in_one = ' '.join([doc for doc in documents_up_famnit.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_up_famnit = up_famnit_corpus_all_in_one.lower()  \n",
    "#cleaned_text_up_famnit = re.sub('[^a-žA-Ž]', ' ', cleaned_text_up_famnit )  \n",
    "cleaned_text_up_famnit = re.sub(r'\\s+', ' ', cleaned_text_up_famnit)\n",
    "end=timer()\n",
    "print(\"time required for reading UP FAMNIT files:\", end-start)\n",
    "#############################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################### -- UNIVERZA NOVO MESTO, FAKULTETA ZA POSLOVNE IN UPRAVNE VEDE -- ####################\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUNM_FPUV') #Get data folder \n",
    "all_unm_fpuv_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_unm_fpuv_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_unnm_fpuv[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "unm_fpuv_corpus_all_in_one = ' '.join([doc for doc in documents_unnm_fpuv.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_unm_fpuv = unm_fpuv_corpus_all_in_one.lower()  \n",
    "#cleaned_text_unm_fpuv = re.sub('[^a-žA-Ž]', ' ', cleaned_text_unm_fpuv )  \n",
    "cleaned_text_unm_fpuv = re.sub(r'\\s+', ' ', cleaned_text_unm_fpuv)\n",
    "end=timer()\n",
    "print(\"time required for reading UNNM FPUV files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "#################################### -- UNIVERZA NOVO MESTO, FAKULTETA ZA ZDRAVSTVENE VEDE -- ###############\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUNM_FZV') #Get data folder \n",
    "all_unm_fzv_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_unm_fzv_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_unnm_fzv[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "unm_fzv_corpus_all_in_one = ' '.join([doc for doc in documents_unnm_fzv.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_unm_fzv = unm_fzv_corpus_all_in_one.lower()  \n",
    "#cleaned_text_unm_fzv = re.sub('[^a-žA-Ž]', ' ', cleaned_text_unm_fzv )  \n",
    "cleaned_text_unm_fzv = re.sub(r'\\s+', ' ', cleaned_text_unm_fzv)\n",
    "end=timer()\n",
    "print(\"time required for reading UNNM FZV files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "\n",
    "#################################### -- UNIVERZA NOVO MESTO, FAKULTETA ZA EKONOMIJO IN INFORMATIKO DATA FOLDER -- #########\n",
    "start=timer()\n",
    "#prepare data folder where documents are located\n",
    "data_folder = os.path.join('Pravilniki/AktiUNM_FEI') #Get data folder \n",
    "all_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)] #read all files\n",
    "\n",
    "#read all documents and save them to dictionary\n",
    "for file in all_files:\n",
    "    bname = os.path.basename(file)\n",
    "    document_number = os.path.splitext(bname)[0]\n",
    "    with open(file, 'rb') as f:\n",
    "        extracted_text = slate.PDF(f).text(clean=True) #cleans the text of unusual text chars\n",
    "        extracted_text = ''.join(i for i in extracted_text if not i in bad_chars) #clean the text of _\n",
    "        documents_unnm_fei[document_number] = extracted_text #document is a dictionary\n",
    "        \n",
    "# save text from all docs to one variable --> corpus all in one is a type of string which contains text from both documents\n",
    "unm_fei_corpus_all_in_one = ' '.join([doc for doc in documents_unnm_fei.values()])\n",
    "\n",
    "#As we can see above we still have some strange figures in text, need more cleaning\n",
    "cleaned_text_unm_fei = unm_fei_corpus_all_in_one.lower()  \n",
    "#cleaned_text_unm_fei = re.sub('[^a-žA-Ž]', ' ', cleaned_text_unm_fei )  \n",
    "cleaned_text_unm_fei = re.sub(r'\\s+', ' ', cleaned_text_unm_fei)\n",
    "\n",
    "end=timer()\n",
    "print(\"time required for reading UNNM FEI files:\", end-start)\n",
    "#############################################################################################################\n",
    "\n",
    "end_common=timer()\n",
    "print(\"time required for all files:\", end_common-start_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save all documents as dictioanry for later use in other ipynb notebook\n",
    "# here we are saving documents as dictionary where key for each document is document name and value is content of document\n",
    "\n",
    "# ************************************ UM FILES *********************************************\n",
    "with open('PDFReaderSavedData/documents_um.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_um, filehandle)  \n",
    "    \n",
    "with open('PDFReaderSavedData/documents_um_feri.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_um_feri, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_um_fzv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_um_fzv, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_um_pef.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_um_pef, filehandle)    \n",
    "    \n",
    "# ************************************ UL FILES *********************************************\n",
    "with open('PDFReaderSavedData/documents_ul.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_ul, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_ul_pef.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_ul_pef, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_ul_fri.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_ul_fri, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_ul_fzv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_ul_fzv, filehandle)\n",
    "    \n",
    "    \n",
    "# ************************************ UP FILES *********************************************   \n",
    "with open('PDFReaderSavedData/documents_up.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_up, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_up_pef.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_up_pef, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_up_famnit.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_up_famnit, filehandle)        \n",
    "    \n",
    "# ************************************ UNNM FILES *********************************************\n",
    "with open('PDFReaderSavedData/documents_unnm_fpuv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_unnm_fpuv, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_unnm_fzv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_unnm_fzv, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/documents_unnm_fei.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_unnm_fei, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are saving simplpe object (string) containing content for each faculty separrately\n",
    "\n",
    "# ************************************ UM FILES *********************************************\n",
    "with open('PDFReaderSavedData/cleaned_text_um.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_um, filehandle)  \n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_um_feri.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_um_feri, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_um_fzv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_um_fzv, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_um_pef.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_um_pef, filehandle)   \n",
    "    \n",
    "# ************************************ UL FILES *********************************************\n",
    "with open('PDFReaderSavedData/cleaned_text_ul.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_ul, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_ul_pef.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_ul_pef, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_ul_fri.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_ul_fri, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_ul_fzv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_ul_fzv, filehandle)   \n",
    "    \n",
    "# ************************************ UP FILES *********************************************   \n",
    "with open('PDFReaderSavedData/cleaned_text_up.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_up, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_up_pef.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_up_pef, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_up_famnit.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_up_famnit, filehandle)        \n",
    "    \n",
    "# ************************************ UNNM FILES *********************************************\n",
    "with open('PDFReaderSavedData/cleaned_text_unm_fpuv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_unm_fpuv, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_unm_fzv.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_unm_fzv, filehandle)\n",
    "    \n",
    "with open('PDFReaderSavedData/cleaned_text_unm_fei.txt', 'wb') as filehandle:\n",
    "    pickle.dump(cleaned_text_unm_fei, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets combine also cleaned text into one string (used in \"Bag of words\" graph for example)\n",
    "cleaned_text_universities_joined = cleaned_text_um + \" \" + cleaned_text_um_feri + \" \" + cleaned_text_um_fzv + cleaned_text_um_pef + \\\n",
    "\" \" + cleaned_text_ul + \" \" + cleaned_text_ul_pef + \" \" + cleaned_text_ul_fri + \" \" + cleaned_text_ul_fzv + \\\n",
    "\" \" + cleaned_text_up + \" \" + cleaned_text_up_pef + \" \" + cleaned_text_up_famnit + \\\n",
    "\" \" + cleaned_text_unm_fpuv + \" \" + cleaned_text_unm_fzv + \" \" + cleaned_text_unm_fei\n",
    "\n",
    "with open('PDFReaderSavedData/cleaned_text_universities_joined.txt', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(cleaned_text_universities_joined, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will merge dictionaries of faculties in one common big dictionary (example: needed in Doc2Vec training).\n",
    "# Each document name represents key and content is value\n",
    "\n",
    "#method that merges dictionaries together\n",
    "def Merge(dict1, dict2): \n",
    "    return(dict1.update(dict2)) \n",
    "\n",
    "documents_dicts_merged={}\n",
    "Merge(documents_dicts_merged, documents_um)\n",
    "Merge(documents_dicts_merged, documents_um_feri)\n",
    "Merge(documents_dicts_merged, documents_um_fzv)\n",
    "Merge(documents_dicts_merged, documents_um_pef)\n",
    "Merge(documents_dicts_merged, documents_ul)\n",
    "Merge(documents_dicts_merged, documents_ul_pef)\n",
    "Merge(documents_dicts_merged, documents_ul_fri)\n",
    "Merge(documents_dicts_merged, documents_ul_fzv)\n",
    "Merge(documents_dicts_merged, documents_up)\n",
    "Merge(documents_dicts_merged, documents_up_pef)\n",
    "Merge(documents_dicts_merged, documents_up_famnit)\n",
    "Merge(documents_dicts_merged, documents_unnm_fpuv)\n",
    "Merge(documents_dicts_merged, documents_unnm_fzv)\n",
    "Merge(documents_dicts_merged, documents_unnm_fei)\n",
    "\n",
    "with open('PDFReaderSavedData/dictionary_of_documents.txt', 'wb') as filehandle:\n",
    "    pickle.dump(documents_dicts_merged, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but here we will save same data into list instead into dictionary, each index here is saving content \n",
    "# data is a list of document values where wach index in list represents document text\n",
    "data=list(documents_um.values()) + list(documents_um_feri.values()) + list(documents_um_fzv.values()) + list(documents_um_pef.values()) + \\\n",
    "list(documents_ul.values()) + list(documents_ul_pef.values()) + list(documents_ul_fri.values()) + list(documents_ul_fzv.values()) + \\\n",
    "list(documents_up.values()) + list(documents_up_pef.values()) + list(documents_up_famnit.values()) + \\\n",
    "list(documents_unnm_fpuv.values()) + list(documents_unnm_fzv.values()) + list(documents_unnm_fei.values())\n",
    "  \n",
    "with open('PDFReaderSavedData/list_of_documents.txt', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data, filehandle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
